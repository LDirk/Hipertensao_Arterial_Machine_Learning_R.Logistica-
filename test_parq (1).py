# -*- coding: utf-8 -*-
"""test_parq.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lGnpPzLmbDLgG4KD-7aIFSH3imG-v4q1
"""

from google.colab import drive

import time

drive.mount('/content/drive')



from pyspark.sql.functions import avg, year, lag, col
from pyspark.sql.window import Window

#Instalar as dependências

#Instalar Java 8
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

#Realizar o download do Spark
!wget -q https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz

#Descompartar o arquivo baixado
!tar xf spark-3.4.0-bin-hadoop3.tgz

#Instalando a findspark
!pip install -q findspark

#Configurar as variáveis de ambiente

#Importando a biblioteca os
import os

#Definindo a variável de ambiente do Java
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

#Definindo a variável de ambiente do Spark
os.environ["SPARK_HOME"] = "/content/spark-3.4.0-bin-hadoop3"

#Importando a findspark
import findspark

#Iniciando o findspark
findspark.init('spark-3.4.0-bin-hadoop3')

# iniciar uma sessão local
from pyspark.sql import SparkSession

import time
from pyspark.sql.functions import *
sc = SparkSession.builder.master('local[*]').config('spark.ui.port', '4050').getOrCreate()

sc

# Iniciar contagem de tempo
start_time = time.time()

df = sc.read.csv("/content/drive/MyDrive/arquivo_carga", sep=';', header=True)

# Calcular o tempo de carregamento

end_time = time.time()
total_time = end_time - start_time

# Imprimir o tempo de carregamento
print("Tempo de carregamento utilizando Spark : %.2f segundos" % total_time)

df.createOrReplaceTempView("temp_table")
df_base = sc.sql("SELECT  TO_DATE( `Data da Coleta`, 'dd/MM/yyyy' ) AS data, `Regiao - Sigla` regiao,`Estado - Sigla` uf,`municipio`,`Bandeira` bandeira,`CNPJ da Revenda` cnpj,`produto` produto, CAST(REPLACE(`Valor de Venda`, ',', '.') AS FLOAT) venda FROM temp_table")
#df_base

# Extrair o ano da coluna de data
df_base = df_base.withColumn("Ano", year(df_base["Data"]))

# Obter os anos distintos em ordem decrescente
anos_distintos = df_base.select("Ano").distinct().orderBy("Ano", ascending=False)

# Exibir os anos distintos em ordem decrescente
anos_distintos.show()

df_base.count()

df_base.show()

df_base.write.mode('overwrite').parquet("/content/drive/MyDrive/Silver")

# Iniciar contagem de tempo
start_time = time.time()

df = sc.read.parquet("/content/drive/MyDrive/Silver")

# Calcular o tempo de carregamento

end_time = time.time()
total_time = end_time - start_time

# Imprimir o tempo de carregamento
print("Tempo de carregamento utilizando Spark : %.2f segundos" % total_time)

# Extrair o ano da coluna de data
df = df.withColumn("Ano", year(df["Data"]))

# Obter os anos distintos em ordem decrescente
anos_distintos = df.select("Ano").distinct().orderBy("Ano", ascending=False)

# Exibir os anos distintos em ordem decrescente
anos_distintos.show()

df.count()

# Converter a coluna de data para o formato adequado
df_base = df.withColumn("Data", to_date(df["Data"], "yyyy-MM-dd"))

# Agrupar por ano e calcular a média de venda
df_media_venda = df_base.groupBy(year("Data").alias("Ano")).agg(avg("Venda").alias("Media_Venda"))

# Definir a janela para cálculo da variação
window = Window.orderBy("Ano")

# Calcular a variação da média de um ano para o outro
df_variacao_media = df_media_venda.withColumn("Variacao_Media", (col("Media_Venda") - lag("Media_Venda").over(window))/lag("Media_Venda").over(window))

# Criar o novo DataFrame com as colunas desejadas
df_novo = df_media_venda.join(df_variacao_media.select("Ano", "Variacao_Media"), "Ano")

# Exibir o novo DataFrame
df_novo.show()

# Converter a coluna de data para o formato adequado
df_base = df.withColumn("Data", to_date(df["Data"], "yyyy-MM-dd"))

# Filtrar os produtos desejados ('GNV', 'GASOLINA', 'ETANOL' e 'DIESEL')
produtos_desejados = ['GNV', 'GASOLINA', 'ETANOL', 'DIESEL']
df_filtrado = df_base.filter(col("produto").isin(produtos_desejados))

# Agrupar por ano e produto, e calcular a média de venda
df_media_venda = df_filtrado.groupBy(year("Data").alias("Ano"), "produto").agg(avg("Venda").alias("Media_Venda"))

# Definir a janela para cálculo da variação
window = Window.partitionBy("produto").orderBy("Ano")

# Calcular a variação da média de um ano para o outro para cada produto
df_variacao_media = df_media_venda.withColumn("Variacao_Media", (col("Media_Venda") - lag("Media_Venda").over(window)) / lag("Media_Venda").over(window))

# Criar o novo DataFrame com as colunas desejadas
df_novo = df_media_venda.join(df_variacao_media.select("Ano", "produto", "Variacao_Media"), ["Ano", "produto"])

# Exibir o novo DataFrame
df_novo.show(5)

import pandas as pd

# Converter o DataFrame Spark para Pandas DataFrame
df_pandas = df_novo.toPandas()

import matplotlib.pyplot as plt

# Criar uma figura e um eixo com tamanho personalizado
fig, ax = plt.subplots(figsize=(12, 6))  # Definir a largura (12) e altura (6) da figura

# Iterar pelos produtos únicos e plotar as séries temporais com cores diferentes
for produto in df_pandas['produto'].unique():
    df_produto = df_pandas[df_pandas['produto'] == produto]
    df_produto['Ano'] = df_produto['Ano'].astype(int).astype(str).str[:4]  # Converter para string com 4 primeiros dígitos

    if produto == 'GASOLINA':
        ax.plot(df_produto['Ano'], df_produto['Media_Venda'], label=produto)

        # Adicionar rótulos aos pontos de dados
        for i, (ano, media_venda) in enumerate(zip(df_produto['Ano'], df_produto['Media_Venda'])):
            ax.annotate(f'{media_venda:.2f}', (ano, media_venda), textcoords="offset points", xytext=(0,5), ha='center')
    else:
        ax.plot(df_produto['Ano'], df_produto['Media_Venda'], label=produto)

# Configurar o título e as legendas
ax.set_title('Gráfico 1 - Série Temporal da Média de Venda por Produto',fontsize=16)
ax.legend()

ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_visible(False)

# Exibir o gráfico
plt.show()

# Filtrar as linhas com data_coleta maior que 2019
df_filtered = df_base.filter(df_base.Data >= '2022-01-01')

import pyspark.sql.functions as F

# Calcular estatísticas descritivas da coluna valor_venda agrupado por cidade
df_stats = df_filtered.groupBy('municipio').agg(
    F.mean('venda').alias('mean'),
    F.expr('percentile_approx(venda, 0.25)').alias('q1'),
    F.expr('percentile_approx(venda, 0.5)').alias('median'),
    F.expr('percentile_approx(venda, 0.75)').alias('q3'),
    F.stddev('venda').alias('std_dev')
)

# Converter o DataFrame do Spark para um DataFrame do pandas
df_pandas = df_stats.toPandas()

df_pandas.sort_values(by='mean', ascending=False).head(5)

df_pandas.sort_values(by='std_dev', ascending=False).head(5)

# Todos os anos
df_filtered2 = df_base


# Calcular estatísticas descritivas da coluna valor_venda agrupado por cidade
df_stats2 = df_filtered2.groupBy('municipio').agg(
    F.mean('venda').alias('mean'),
    F.expr('percentile_approx(venda, 0.25)').alias('q1'),
    F.expr('percentile_approx(venda, 0.5)').alias('median'),
    F.expr('percentile_approx(venda, 0.75)').alias('q3'),
    F.stddev('venda').alias('std_dev')
)

# Converter o DataFrame do Spark para um DataFrame do pandas
df_pandas2 = df_stats2.toPandas()

df_pandas2.sort_values(by='mean', ascending=False).head(5)

df_pandas2.sort_values(by='std_dev', ascending=False).head(5)

df_filtered.show(5)

# AGRUPAR POR REGIAO GEOGRAFICA, gasolina e ano = 2022

df_base = df.withColumn("Data", to_date(df["Data"], "yyyy-MM-dd"))

# Calcular estatísticas descritivas da coluna valor_venda agrupado por cidade, filtrando apenas o produto "gasolina"
df_stats = df_filtered.filter((df_filtered.produto == 'GASOLINA') & (df_filtered.Ano == '2022')).groupBy('regiao').agg(
    F.mean('venda').alias('mean'),
    F.expr('percentile_approx(venda, 0.25)').alias('q1'),
    F.expr('percentile_approx(venda, 0.5)').alias('median'),
    F.expr('percentile_approx(venda, 0.75)').alias('q3'),
    F.stddev('venda').alias('std_dev')
)

# Converter o DataFrame do Spark para um DataFrame do pandas
df_pandas3 = df_stats.toPandas()

df_pandas3.sort_values(by='mean', ascending=False).head(5)

# Configurar o gráfico
plt.figure(figsize=(10, 6)) # Tamanho da figura
plt.errorbar(df_pandas3['regiao'], df_pandas3['mean'], yerr=df_pandas3['std_dev'], fmt='o', color='red', capsize=4, label='Desvio Padrão')
plt.plot(df_pandas3['regiao'], df_pandas3['mean'], linestyle='-', marker='o', color='blue', label='Média do valor da Gasolina')

# Adicionar rótulos dos valores do desvio padrão
for i in range(len(df_pandas3)):
    plt.text(df_pandas3['regiao'][i], df_pandas3['mean'][i]+0.15, f"{df_pandas3['std_dev'][i]:.2f}", ha='center', va='bottom', color='red')

# Adicionar rótulos dos valores da média
for i in range(len(df_pandas3)):
    plt.text(df_pandas3['regiao'][i], df_pandas3['mean'][i] - 0.15, f"{df_pandas3['mean'][i]:.2f}", ha='center', va='top', color='blue')



# Títulos e legendas
plt.title('Gráfico 2 - Estatísticas Da média de preço da Gasolina por Região')
plt.xlabel('Região')
plt.ylabel('Valor')
plt.legend()

# Ajustar layout para melhor visibilidade dos rótulos
plt.tight_layout()



# Exibir o gráfico
plt.show()
fig.savefig('esta.png')



# AGRUPAR POR ESTADO

# Calcular estatísticas descritivas da coluna valor_venda agrupado por cidade
df_stats = df_filtered.groupBy('uf').agg(
    F.mean('venda').alias('mean'),
    F.expr('percentile_approx(venda, 0.25)').alias('q1'),
    F.expr('percentile_approx(venda, 0.5)').alias('median'),
    F.expr('percentile_approx(venda, 0.75)').alias('q3'),
    F.stddev('venda').alias('std_dev')
)

# Converter o DataFrame do Spark para um DataFrame do pandas
df_pandas4 = df_stats.toPandas()

df_pandas4.sort_values(by='mean', ascending=False).head(100)