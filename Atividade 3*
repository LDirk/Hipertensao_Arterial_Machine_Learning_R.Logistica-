### Carregando as bibliotecas 

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import statsmodels.api as sm

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.utils import class_weight
from sklearn.metrics import roc_curve, roc_auc_score, precision_score, accuracy_score,confusion_matrix


### Dicionário dos dados
'''
gender: genero
age: idade
hyptertension: hipertensão (Binário)
heart_disease: doença cardiáca
ever_married: Se o paciente já foi casado
work_type: Tipo de trabalho de um paciente. 
Residence_type: tipo de residência 
avg_glucose_level: nível médio de glicose no sangue. 
bmi: imc
smoking_status: Se já fumou, fuma ou nunca fumou
stroke: se já teve AVC (Binário)
'''

## Objetivo:
'''
Objetivo: Prever a ocorrência de AVC (Stroke)
'''

### Abrindo o dataframe
colunas = ['gender','age','hypertension','heart_disease','work_type','Residence_type','avg_glucose_level','bmi','smoking_status','stroke']

df = pd.read_csv('/content/healthcare-dataset-stroke-data.csv', sep = ',' , usecols= colunas)

###############################################################################################################################################################################################

### Análise exploratória dos dados e pré processamento. 

## Verificar se as variáveis são correlacionadas entre si. 
matriz_correlacao = df.corr()
matriz_correlacao
# Observamos que as variáveis não são correlacionadas, assim evitanto o problema de multicolinearidade

## Verificando o tamanho do dataset. 
len(df)
# 5110 linhas

## Verificando se existem dados faltantes
df.isnull().sum()
# Existem 201 dados faltantes na coluna bmi (IMC)

# Como 201 dados são poucos na comparando com o número de registro, apenas irei remove-lós sem utilizar técnicas de imputação de dados: 
df = df.dropna()

## Convertento as variáveis discretas em uma representação numérica

mapeamento_genero= {'Male': 1, 'Female': 0, 'Other': 99}
mapeamento_work = {'Private':0 , 'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4}
mapeamento_Residence = {'Urban': 1 , 'Rural': 0}
mapeamento_smok = {'formerly smoked': 0, 'never smoked': 1, 'smokes': 2, 'Unknown': 3}

df['gender'] = df['gender'].map(mapeamento_genero)
df['work_type'] = df['work_type'].map(mapeamento_work)
df['Residence_type'] = df['Residence_type'].map(mapeamento_Residence)
df['smoking_status'] = df['smoking_status'].map(mapeamento_smok)

## Observamos que as variáveis age, avg_glucose_level e bmi possuem escalas bem diferentes, então, vão ser normalizadas. 

colunas = ['age', 'avg_glucose_level', 'bmi']
scaler = StandardScaler()
df[colunas] = scaler.fit_transform(df[colunas])

## Verificando se a variavel target está balanceada
contagem = df['stroke'].value_counts()
contagem 

# A variável target não está balanceada, será necessário utilizar alguma técnica de balanceamento para evitarmos problemas devido a esse desbalanceamento.

## Irei utilizar como teste duas técnicas de balancamento, Undersampling e a técnica do ajuste de pesos, com o objetivo de ver qual possui a melhor perfomance. 

# Undersampling

contagem_classes = df['stroke'].value_counts()
min_observacoes = contagem_classes.min()
df_1 = df.groupby('stroke').apply(lambda x: x.sample(min_observacoes)).reset_index(drop=True)

# A juste dos pesos

weights = class_weight.compute_class_weight('balanced', classes=sorted(df['stroke'].unique()), y=df['stroke'])
class_weights = {i: weights[i] for i in range(len(weights))}
df_2 = pd.concat([df[df['stroke'] == classe].sample(frac=class_weights[classe], replace=True) for classe in df['stroke'].unique()])
df_2.reset_index(drop=True, inplace=True)

###############################################################################################################################################################################################

### Construção dos modelos de regressão logística











































































######################################################################################################################################################################################################################
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import confusion_matrix
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

'''
gender: genero
age: idade
hyptertension: hipertensão (Binário)
heart_disease: doença cardiáca
ever_married: Se o paciente já foi casado
work_type: Tipo de trabalho de um paciente. 
Residence_type: tipo de residência 
avg_glucose_level: nível médio de glicose no sangue. 
bmi: imc
smoking_status: Se já fumou, fuma ou nunca fumou
stroke: se já teve AVC (Binário)
'''

'''
Objetivo: Prever a ocorrência de AVC
'''

colunas = ['gender','age','hypertension','heart_disease','work_type','Residence_type','avg_glucose_level','bmi','smoking_status','stroke']
df = pd.read_csv('/content/healthcare-dataset-stroke-data.csv', sep = ',' , usecols= colunas)
df.head()

# Verificando se existem dados faltantes.
df.isnull().sum()

# Removendo valores faltantes (São poucos se comparado ao numero total de registros)
df = df.dropna()

# Convertendo textos para binários

mapeamento_genero= {'Male': 1, 'Female': 0, 'Other': 99}
mapeamento_work = {'Private':0 , 'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4}
mapeamento_Residence = {'Urban': 1 , 'Rural': 0}
mapeamento_smok = {'formerly smoked': 0, 'never smoked': 1, 'smokes': 2, 'Unknown': 3}

df['gender'] = df['gender'].map(mapeamento_genero)
df['work_type'] = df['work_type'].map(mapeamento_work)
df['Residence_type'] = df['Residence_type'].map(mapeamento_Residence)
df['smoking_status'] = df['smoking_status'].map(mapeamento_smok)


# Normalizando os dados numéricos. 

# age, Normalizando os dados, bmi
colunas = ['age', 'avg_glucose_level', 'bmi']
scaler = StandardScaler()
df[colunas] = scaler.fit_transform(df[colunas])



# Ajustando o modelo de regressão logística
model = sm.Logit(df['stroke'], df[['intercept', 'gender','age','hypertension','heart_disease','work_type','Residence_type','avg_glucose_level','bmi','smoking_status']])

result = model.fit()

# Obtendo o resumo da regressão logística
summary = result.summary()

# Imprimindo o resumo
print(summary)

# removendo as variaveis ruins 
X = df.drop(['stroke', 'gender', 'Residence_type', 'bmi', 'smoking_status'], axis=1)
y = df['stroke'] # Variável target.


predictions = model.predict(X)

accuracy = accuracy_score(y, predictions)
print("Acurácia:", accuracy)
